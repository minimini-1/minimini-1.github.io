---
title: "A Training-Free Style-Personalization via SVD-Based Feature Decomposition"
description: arXiv preprint
author: Kyoungmin Lee*, Jihun Park*, <strong>Jongmin Gim*</strong>, Wonhyeok Choi, Kyumin Hwang, Jaeyeul Kim, and Sunghoon Im.
categories: publications
date: 2025-11-30
pin: false
math: true
mermaid: true
image:
  path: /assets/img/post_images/svd-style.png
  lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
  alt: SVD-Style
---

## Abstract

We present a training-free framework for style-personalized image generation that operates during inference using a scale-wise autoregressive model. Our method generates a stylized image guided by a single reference style while preserving semantic consistency and mitigating content leakage. Through a detailed step-wise analysis of the generation process, we identify a pivotal step where the dominant singular values of the internal feature encode style-related components. Building upon this insight, we introduce two lightweight control modules: Principal Feature Blending, which enables precise modulation of style through SVD-based feature reconstruction, and Structural Attention Correction, which stabilizes structural consistency by leveraging content-guided attention correction across fine stages. Without any additional training, extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.

## Links

- **Paper:** [arXiv](https://arxiv.org/abs/2507.04482)

## Citation

```bibtex
@article{lee2025svdstyle,
  title={A Training-Free Style-Personalization via SVD-Based Feature Decomposition},
  author={Lee, Kyoungmin and Park, Jihun and Gim, Jongmin and Choi, Wonhyeok and Hwang, Kyumin and Kim, Jaeyeul and Im, Sunghoon},
  journal={arXiv preprint arXiv:2507.04482},
  year={2025}
}
```
